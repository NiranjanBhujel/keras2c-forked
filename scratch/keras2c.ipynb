{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### imports\n",
    "import numpy as np\n",
    "import keras\n",
    "maxndim = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "### array2c\n",
    "def array2c(array,name):\n",
    "    temp = array.flatten(order='C')\n",
    "    size = array.size\n",
    "    shp = array.shape\n",
    "    ndim = len(shp)\n",
    "    shp = np.concatenate((shp,np.ones(maxndim-ndim)))\n",
    "    count = 0\n",
    "    s = 'float ' + name + '_array[' + str(size) + '] = ' \n",
    "    if np.max(np.abs(temp))<1e-16:\n",
    "        s += '{' + str(0) + '}; \\n'\n",
    "    else:\n",
    "        s += '{\\n'\n",
    "        for i in range(size):\n",
    "            if temp[i] == np.inf:\n",
    "                s += \"HUGE_VAL,\"\n",
    "            elif temp[i] == -np.inf:\n",
    "                s += \"-HUGE_VAL,\"\n",
    "            else:\n",
    "                s += \"{:.10e}\".format(temp[i]) + ','\n",
    "            count += 1\n",
    "            if (count)%4 is 0:\n",
    "                s += '\\n'\n",
    "        s += '}; \\n'\n",
    "    s += 'k2c_tensor ' + name + ' = {&' + name + '_array[0],' + str(int(ndim)) + ',' + str(int(size)) + ',{' + \\\n",
    "        np.array2string(shp.astype(int),separator=',')[1:-1] + '}}; \\n'\n",
    "    return s    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### weights2c\n",
    "\n",
    "def write_outputs(layer,file,model_io):\n",
    "    inputs, outputs = get_layer_io_names(layer)\n",
    "    for i,outp in enumerate(outputs):\n",
    "        outshp = layer.get_output_at(i).shape[1:]\n",
    "        if outp not in model_io[1]:\n",
    "            file.write(array2c(np.zeros(outshp),outp + '_output'))\n",
    "                        \n",
    "def write_weights_LSTM(layer,file, model_io):\n",
    "    units = layer.get_config()['units'] \n",
    "    write_outputs(layer,file,model_io)\n",
    "    s = 'float ' + layer.name + '_fwork[' + str(8*units) + '] = {0}; \\n'\n",
    "    s += 'int ' + layer.name + '_go_backwards = ' + \\\n",
    "        str(int(layer.get_config()['go_backwards'])) + ';\\n'\n",
    "    s += 'int ' + layer.name + '_return_sequences = ' + \\\n",
    "        str(int(layer.get_config()['return_sequences'])) + ';\\n'\n",
    "    s += 'float ' + layer.name + '_state[' + str(2*units) + '] = {0}; \\n'\n",
    "    file.write(s)\n",
    "\n",
    "    weights = layer.get_weights()\n",
    "    kernel = weights[0]\n",
    "    recurrent_kernel = weights[1]\n",
    "    if layer.get_config()['use_bias']:\n",
    "        bias = weights[2]\n",
    "    else:\n",
    "        bias = np.zeros(4*units)\n",
    "    ckernel = np.concatenate([kernel[:,:units],\\\n",
    "                              kernel[:,units:2*units],\\\n",
    "                              kernel[:,2*units:3*units],\\\n",
    "                              kernel[:,3*units:]],axis=0)\n",
    "    crecurrent_kernel = np.concatenate([recurrent_kernel[:,:units],\\\n",
    "                                        recurrent_kernel[:,units:2*units],\\\n",
    "                                        recurrent_kernel[:,2*units:3*units],\\\n",
    "                                        recurrent_kernel[:,3*units:]],axis=0)\n",
    "    file.write(array2c(ckernel,layer.name + '_kernel'))\n",
    "    file.write(array2c(crecurrent_kernel,layer.name + '_recurrent_kernel'))\n",
    "    file.write(array2c(bias,layer.name + '_bias'))\n",
    "    file.write('\\n \\n')\n",
    "    \n",
    "def write_weights_GRU(layer,file, model_io):\n",
    "    units = layer.get_config()['units']\n",
    "    write_outputs(layer,file,model_io)\n",
    "    s = 'float ' + layer.name + '_fwork[' + str(6*units) + '] = {0}; \\n'\n",
    "    s += 'int ' + layer.name + '_reset_after = ' + \\\n",
    "        str(int(layer.get_config()['reset_after'])) + ';\\n'\n",
    "    s += 'int ' + layer.name + '_go_backwards = ' + \\\n",
    "        str(int(layer.get_config()['go_backwards'])) + ';\\n'\n",
    "    s += 'int ' + layer.name + '_return_sequences = ' + \\\n",
    "        str(int(layer.get_config()['return_sequences'])) + ';\\n'\n",
    "    s += 'float ' + layer.name + '_state[' + str(units) + '] = {0}; \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "    weights = layer.get_weights()   \n",
    "    kernel = weights[0]\n",
    "    recurrent_kernel = weights[1]\n",
    "    if layer.get_config()['use_bias']:\n",
    "        bias = weights[2]\n",
    "        if layer.get_config()['reset_after']:\n",
    "            bias = b[0]\n",
    "            rbias = b[1]\n",
    "        else:\n",
    "            bias = bias\n",
    "            rbias = np.zeros(3*units)\n",
    "    else:\n",
    "        bias = np.zeros(3*units)\n",
    "        rbias = np.zeros(3*units)\n",
    "    bias = np.concatenate([bias,rbias],axis=0)\n",
    "    ckernel = np.concatenate([kernel[:,:units],\\\n",
    "                              kernel[:,units:2*units],\\\n",
    "                              kernel[:,2*units:]],axis=0)\n",
    "    crecurrent_kernel = np.concatenate([recurrent_kernel[:,:units],\\\n",
    "                                        recurrent_kernel[:,units:2*units],\n",
    "                                        recurrent_kernel[:,2*units:3*units]],axis=0)\n",
    "    file.write(array2c(ckernel,layer.name + '_kernel'))\n",
    "    file.write(array2c(crecurrent_kernel,layer.name + '_recurrent_kernel'))\n",
    "    file.write(array2c(bias,layer.name + '_bias'))\n",
    "    file.write('\\n \\n')    \n",
    "\n",
    "def write_weights_SimpleRNN(layer,file, model_io):\n",
    "    units = layer.get_config()['units']\n",
    "    write_outputs(layer,file,model_io)\n",
    "    s = 'int ' + layer.name + '_go_backwards = ' + \\\n",
    "        str(int(layer.get_config()['go_backwards'])) + ';\\n'\n",
    "    s += 'int ' + layer.name + '_return_sequences = ' + \\\n",
    "        str(int(layer.get_config()['return_sequences'])) + ';\\n'\n",
    "    s += 'float ' + layer.name + '_fwork[' + str(2*units) + '] = {0}; \\n'\n",
    "    s += 'float ' + layer.name + '_state[' + str(units) + '] = {0}; \\n'\n",
    "    file.write(s)\n",
    "\n",
    "    weights = layer.get_weights()\n",
    "    kernel = weights[0]\n",
    "    recurrent_kernel = weights[1]\n",
    "    if layer.get_config()['use_bias']:\n",
    "        bias = weights[2]\n",
    "    else:\n",
    "        bias = np.zeros(units)\n",
    "    file.write(array2c(kernel,layer.name + '_kernel'))\n",
    "    file.write(array2c(recurrent_kernel,layer.name + '_recurrent_kernel'))\n",
    "    file.write(array2c(bias,layer.name + '_bias'))\n",
    "    file.write('\\n \\n')    \n",
    "        \n",
    "def write_weights_Dense(layer,file, model_io):\n",
    "    write_outputs(layer,file,model_io)\n",
    "    weights = layer.get_weights()\n",
    "    A = weights[0]\n",
    "    if layer.get_config()['use_bias']:\n",
    "        b = weights[1]\n",
    "    else:\n",
    "        b = np.zeros(A.shape[1])\n",
    "\n",
    "    file.write(array2c(A,layer.name + '_kernel'))\n",
    "    file.write(array2c(b,layer.name + '_bias'))\n",
    "    s = 'float ' + layer.name + '_fwork[' + str(np.prod(layer.input_shape[1:])+np.prod(A.shape)) + '] = {0}; \\n'\n",
    "    file.write(s)\n",
    "    file.write('\\n \\n')\n",
    "\n",
    "def write_weights_Conv1D(layer,file, model_io):\n",
    "    pad = layer.get_config()['padding']\n",
    "    stride = layer.get_config()['strides'][0]\n",
    "    dilation = layer.get_config()['dilation_rate'][0]\n",
    "    kernel_size = layer.get_config()['kernel_size'][0]\n",
    "    s = 'size_t ' + layer.name + '_stride = ' + str(stride) + '; \\n'\n",
    "    s += 'size_t ' + layer.name + '_dilation = ' + str(dilation) + '; \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "    inputs, outputs = get_layer_io_names(layer)\n",
    "    for i,(inp,outp) in enumerate(zip(inputs,outputs)):\n",
    "        inshp = layer.get_input_at(i).shape[1:]\n",
    "        outshp = layer.get_output_at(i).shape[1:]\n",
    "        inrows = inshp[0]\n",
    "        incols = inshp[1]\n",
    "        if pad == 'causal':\n",
    "            pad_along_height = dilation*(kernel_size-1)\n",
    "            pad_top = pad_along_height\n",
    "            pad_bottom = 0\n",
    "        elif pad == 'same':\n",
    "            pad_along_height = max((outshp[0] - 1) * stride*dilation +\n",
    "                    kernel_size - inshp[0], 0)\n",
    "            pad_top = int(pad_along_height // 2)\n",
    "            pad_bottom = int(pad_along_height - pad_top)\n",
    "        elif pad == 'valid':\n",
    "            pad_top=0\n",
    "            pad_bottom=0\n",
    "            \n",
    "        file.write(array2c(np.zeros((inrows+pad_top+pad_bottom,incols)),\\\n",
    "                           layer.name + '_padded' + str(i) + '_input'))\n",
    "        s = 'size_t ' + layer.name + '_pad' + str(i) + '_top = ' + str(pad_top) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_pad' + str(i) + '_bottom = ' + str(pad_bottom) + '; \\n'\n",
    "        s += 'float ' + layer.name + '_fill' + str(i) + ' = 0; \\n'\n",
    "        file.write(s)\n",
    "        if outp not in model_io[1]:\n",
    "            file.write(array2c(np.zeros(outshp),outp + '_output'))\n",
    "\n",
    "    weights = layer.get_weights()\n",
    "    filters = weights[0]\n",
    "    if layer.get_config()['use_bias']:\n",
    "        bias = weights[1]\n",
    "    else:\n",
    "        bias = np.zeros(filters.shape[2]) \n",
    "    file.write(array2c(filters, layer.name + '_kernel'))\n",
    "    file.write(array2c(bias, layer.name + '_bias'))\n",
    "    file.write('\\n \\n')\n",
    "\n",
    "def write_weights_Pooling1D(layer,file, model_io):\n",
    "    pad = layer.get_config()['padding']\n",
    "    stride = layer.get_config()['strides'][0]\n",
    "    pool_size = layer.get_config()['pool_size'][0]\n",
    "    s = 'size_t ' + layer.name + '_stride = ' + str(stride) + '; \\n'\n",
    "    s += 'size_t ' + layer.name + '_pool_size = ' + str(pool_size) + '; \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "    inputs, outputs = get_layer_io_names(layer)\n",
    "    for i,(inp,outp) in enumerate(zip(inputs,outputs)):\n",
    "        inshp = layer.get_input_at(i).shape[1:]\n",
    "        outshp = layer.get_output_at(i).shape[1:]\n",
    "        inrows = inshp[0]\n",
    "        incols = inshp[1]\n",
    "        if pad == 'same':\n",
    "            pad_along_height = max((outshp[0] - 1) * stride +\n",
    "                    pool_size - inshp[0], 0)\n",
    "            pad_top = int(pad_along_height // 2)\n",
    "            pad_bottom = int(pad_along_height - pad_top)\n",
    "        elif pad == 'valid':\n",
    "            pad_top=0\n",
    "            pad_bottom=0 \n",
    "        file.write(array2c(np.zeros((inrows+pad_top+pad_bottom,incols)),\\\n",
    "                           layer.name + '_padded' + str(i) + '_input'))\n",
    "        s = 'size_t ' + layer.name + '_pad' + str(i) + '_top = ' + str(pad_top) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_pad' + str(i) + '_bottom = ' + str(pad_bottom) + '; \\n'\n",
    "        s += 'float ' + layer.name + '_fill' + str(i) + ' = -HUGE_VALF; \\n'\n",
    "        file.write(s)\n",
    "        if outp not in model_io[1]:\n",
    "            file.write(array2c(np.zeros(outshp),outp + '_output'))\n",
    "    \n",
    "    file.write('\\n \\n')\n",
    "    \n",
    "def write_weights_GlobalPooling1D(layer,file, model_io):\n",
    "    write_outputs(layer,file,model_io)\n",
    "    file.write('\\n\\n')   \n",
    "\n",
    "def write_weights_Merge(layer,file, model_io):\n",
    "    inputs, outputs = get_layer_io_names(layer)\n",
    "    for i,(inp,outp) in enumerate(zip(inputs,outputs)):\n",
    "        outshp = layer.get_output_at(i).shape[1:]\n",
    "        num_tensors = len(inp)\n",
    "        s = 'size_t ' + layer.name + '_num_tensors' + str(i) + \\\n",
    "            ' = ' + str(num_tensors) + '; \\n'\n",
    "        file.write(s)\n",
    "        if outp not in model_io[1]:\n",
    "            file.write(array2c(np.zeros(outshp),outp + '_output'))\n",
    "    file.write('\\n\\n')\n",
    "               \n",
    "def write_weights_ELU(layer,file, model_io):\n",
    "    alpha = layer.get_config()['alpha']\n",
    "    s = 'float ' + layer.name + '_alpha = ' + str(alpha) + '; \\n'\n",
    "    file.write(s + '\\n\\n')\n",
    "\n",
    "def write_weights_LeakyReLU(layer,file, model_io):\n",
    "    alpha = layer.get_config()['alpha']\n",
    "    s = 'float ' + layer.name + '_alpha = ' + str(alpha) + '; \\n'\n",
    "    file.write(s + '\\n\\n')\n",
    "\n",
    "def write_weights_ThresholdedReLU(layer,file, model_io):\n",
    "    theta = layer.get_config()['theta']\n",
    "    s = 'float ' + layer.name + '_theta = ' + str(theta) + '; \\n'\n",
    "    file.write(s + '\\n\\n')\n",
    "        \n",
    "def write_weights_ReLU(layer,file, model_io):\n",
    "    max_value = layer.get_config()['max_value']\n",
    "    negative_slope = layer.get_config()['negative_slope']\n",
    "    threshold = layer.get_config()['threshold']\n",
    "    if max_value is None:\n",
    "        max_value = 'HUGE_VALF'\n",
    "    s = 'float ' + layer.name + '_max_value = ' + str(max_value) + '; \\n'\n",
    "    s += 'float ' + layer.name + '_negative_slope = ' + str(negative_slope) + '; \\n'\n",
    "    s += 'float ' + layer.name + '_threshold = ' + str(threshold) + '; \\n' \n",
    "    file.write(s + '\\n\\n')\n",
    "\n",
    "def write_weights_PReLU(layer,file, model_io):\n",
    "    s = array2c(layer.get_weights()[0],layer.name + '_alpha')\n",
    "    file.write(s + '\\n\\n')\n",
    "    \n",
    "def write_weights_Reshape(layer,file,model_io):\n",
    "    nm = layer.name\n",
    "    newshp = model.layers[1].get_config()['target_shape']\n",
    "    newndim = len(newshp)\n",
    "    newshp = np.concatenate((shp,np.ones(maxndim-ndim)))\n",
    "    s = 'size_t ' + nm + '_newndim = ' + str(newndim) + '; \\n'\n",
    "    s += 'size_t' + nm + '_newshp[K2C_MAX_NDIM] = {' + \\\n",
    "        str(np.array2string(shp.astype(int),separator=',')[1:-1]) + '}; \\n'\n",
    "    file.write(s + '\\n\\n')\n",
    "    \n",
    "def write_weights_Permute(layer,file,model_io):\n",
    "    write_outputs(layer,file,model_io)\n",
    "    permute = np.array(layer.get_config()['dims']).astype(int) -1\n",
    "    s = 'size_t ' + layer.name + '_permute[' + str(permute.size) + '] = {' +\\\n",
    "        str(np.array2string(permute.astype(int),separator=',')[1:-1]) + '}; \\n'\n",
    "    file.write(s + '\\n\\n')\n",
    "        \n",
    "def write_weights_RepeatVector(layer,file, model_io):\n",
    "    write_outputs(layer,file,model_io)\n",
    "    n = layer.get_config()['n']\n",
    "    s = 'size_t ' + layer.name + '_n = ' + str(n) + '; \\n'\n",
    "    file.write(s + '\\n\\n')\n",
    "\n",
    "def write_weights_Dot(layer,file,model_io):\n",
    "    write_outputs(layer,file,model_io)\n",
    "    axes = np.array(model.layers[2].get_config()['axes']) -1\n",
    "    s = 'size_t ' + layer.name + '_axesA[1] = {' + str(axes[0]) + '}; \\n'\n",
    "    s += 'size_t ' + layer.name + '_axesB[1] = {' + str(axes[1]) + '}; \\n'\n",
    "    s += 'size_t ' + layer.name + '_naxes = 1; \\n'\n",
    "    s += 'float ' + layer.name + '_fwork[' + str(work_size) + '] = {0}; \\n'\n",
    "    s += 'int ' + nm + '_normalize = ' + str(int(layer.get_config()['normalize'])) + '; \\n'\n",
    "    file.write(s)\n",
    "\n",
    "\n",
    "def weights2c(layer,file, model_io):\n",
    "    if layer_type(layer) == 'Dense':\n",
    "        write_weights_Dense(layer,file, model_io)\n",
    "     \n",
    "    elif layer_type(layer) == 'LSTM':\n",
    "        write_weights_LSTM(layer,file, model_io)\n",
    "        \n",
    "    elif layer_type(layer) == 'GRU':\n",
    "        write_weights_GRU(layer,file, model_io)\n",
    "        \n",
    "    elif layer_type(layer) == 'SimpleRNN':\n",
    "        write_weights_SimpleRNN(layer,file, model_io)\n",
    "\n",
    "    elif layer_type(layer) == 'Conv1D':\n",
    "        write_weights_Conv1D(layer,file, model_io)\n",
    "    \n",
    "    elif layer_type(layer) in ['Add','Subtract','Multiply','Maximum','Minimum','Average']:\n",
    "        write_weights_Merge(layer,file, model_io)\n",
    "\n",
    "    elif layer_type(layer) in ['MaxPooling1D','AveragePooling1D']:\n",
    "        write_weights_Pooling1D(layer,file, model_io)\n",
    "\n",
    "    elif layer_type(layer) in ['GlobalMaxPooling1D','GlobalAveragePooling']:\n",
    "        write_weights_GlobalPooling1D(layer,file, model_io)\n",
    "    \n",
    "    elif layer_type(layer) == 'LeakyReLU':\n",
    "        write_weights_LeakyReLU(layer,file, model_io)\n",
    "        \n",
    "    elif layer_type(layer) == 'ELU':\n",
    "        write_weights_ELU(layer,file, model_io)\n",
    "        \n",
    "    elif layer_type(layer) == 'PReLU':\n",
    "        write_weights_PReLU(layer,file, model_io)\n",
    "        \n",
    "    elif layer_type(layer) == 'ThresholdedReLU':\n",
    "        write_weights_ThresholdedReLU(layer,file, model_io)\n",
    "        \n",
    "    elif layer_type(layer) == 'ReLU':\n",
    "        write_weights_ReLU(layer,file, model_io)\n",
    "\n",
    "    elif layer_type(layer) == 'Reshape':\n",
    "        write_weights_Reshape(layer,file,model_io)\n",
    "        \n",
    "    elif layer_type(layer) == 'Permute':\n",
    "        write_weights_Permute(layer,file,model_io)\n",
    "        \n",
    "    elif layer_type(layer) == 'RepeatVector':\n",
    "        write_weights_RepeatVector(layer,file,model_io)\n",
    "    \n",
    "    elif layer_type(layer) == 'Dot':\n",
    "        write_weights_Dot(layer,file,model_io)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### layer2c\n",
    "\n",
    "def write_layer_LSTM(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    output_activation = 'k2c_' + layer.get_config()['activation']\n",
    "    recurrent_activation = 'k2c_' + layer.get_config()['recurrent_activation']\n",
    "\n",
    "    s = 'k2c_lstm(' + outputs + ',' + inputs + ',' + nm + '_state,' + nm + '_kernel, \\n\\t' + \\\n",
    "        nm + '_recurrent_kernel,' + nm + '_bias,' + nm + '_fwork, \\n\\t' + \\\n",
    "        nm + '_bo_backwards,' + nm + '_return_sequences, \\n\\t' + \\\n",
    "        recurrent_activation + ',' + output_activation + '); \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_Dense(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    activation = 'k2c_' + layer.get_config()['activation']\n",
    "\n",
    "    s = 'k2c_dense(' + outputs + ',' + inputs + ',' + nm + '_kernel, \\n\\t' + \\\n",
    "           nm + '_bias,' + activation + ',' + nm + '_fwork); \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_Conv1D(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    activation = 'k2c_' + layer.get_config()['activation']\n",
    "\n",
    "    s = 'k2c_pad1d(' + nm + '_padded' + str(i) + '_input,' + inputs + ',' + nm + \\\n",
    "        '_fill' + str(i) + ', \\n\\t' + nm + '_pad' + str(i) + '_top,' + nm + '_pad' + str(i) + '_bottom); \\n'\n",
    "    file.write(s)\n",
    "    s = 'k2c_conv1d(' + outputs + ',' + nm + '_padded' + str(i) + '_input,' + nm + '_kernel, \\n\\t' + \\\n",
    "        nm + '_bias,' + nm + '_stride,' + nm + '_dilation,' + activation + '); \\n'\n",
    "    file.write(s)\n",
    "\n",
    "def write_layer_Pooling1D(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    s = 'k2c_pad1d(' + nm + '_padded' + str(i) + '_input,' + inputs + ',' + nm + \\\n",
    "        '_fill' + str(i) + ', \\n\\t' + nm + '_pad' + str(i) + '_top,' + nm + '_pad' + str(i) + '_bottom); \\n'\n",
    "    file.write(s)\n",
    "    if 'Max' in layer_type(layer):\n",
    "        s = 'k2c_maxpool1d('\n",
    "    else:\n",
    "        s = 'k2c_avgpool1d('\n",
    "    s += outputs + ',' + nm + '_padded' + str(i) + '_input,' + nm + '_pool_size, \\n\\t' + \\\n",
    "        nm + '_stride); \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_GlobalPooling1D(layer,file,inputs,outputs,i):\n",
    "    if 'Max' in layer_type(layer):\n",
    "        s = 'k2c_global_max_pooling_1d('\n",
    "    else:\n",
    "        s = 'k2c_global_avg_pooling_1d('\n",
    "    s += outputs + ',' + inputs + '); \\n'\n",
    "    file.write(s)\n",
    "\n",
    "def write_layer_Merge(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name    \n",
    "    if 'Subtract' == layer_type(layer):\n",
    "        s = 'k2c_subtract('\n",
    "    elif 'Add' == layer_type(layer):\n",
    "        s = 'k2c_add('\n",
    "    elif 'Multiply' == layer_type(layer):\n",
    "        s = 'k2c_multiply('\n",
    "    elif 'Average' == layer_type(layer):\n",
    "        s = 'k2c_average('\n",
    "    elif 'Maximum' == layer_type(layer):\n",
    "        s = 'k2c_max('\n",
    "    elif 'Minimum' == layer_type(layer):\n",
    "        s = 'k2c_min('\n",
    "    s += outputs + ',' + nm + '_num_tensors' + str(i) + ',&'\n",
    "    c = ',&'.join(inputs)\n",
    "    s += c + '); \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_GRU(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    output_activation = 'k2c_' + layer.get_config()['activation']\n",
    "    recurrent_activation = 'k2c_' + layer.get_config()['recurrent_activation']\n",
    "\n",
    "    s = 'k2c_gru(' + outputs + ',' + inputs + ',' + nm + '_state,' + nm + '_kernel, \\n\\t' + \\\n",
    "        nm + '_recurrent_kernel,' + nm + '_bias,' + nm + '_fwork, \\n\\t' + \\\n",
    "        nm + '_reset_after,' + nm + '_go_backwards,' + nm + '_return_sequences, \\n\\t' + \\\n",
    "        recurrent_activation + ',' + output_activation + '); \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_SimpleRNN(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    activation = 'k2c_' + layer.get_config()['activation']\n",
    "\n",
    "    s = 'k2c_simpleRNN(' + outputs + ',' + inputs + ',' + nm + '_state,' + nm + '_kernel, \\n\\t' + \\\n",
    "        nm + '_recurrent_kernel,' + nm + '_bias,' + nm + '_fwork, \\n\\t' + \\\n",
    "        nm + '_go_backwards,' + nm + '_return_sequences,' + activation + '); \\n'\n",
    "    file.write(s)    \n",
    "    \n",
    "def write_layer_Activation(layer,file,inputs,outputs,i):\n",
    "    activation = 'k2c_' + layer.get_config()['activation']\n",
    "    s = activation + '(' + inputs + '.array,' + inputs + '.numel); \\n'\n",
    "    s += 'k2c_tensor *' + outputs + ' = ' + inputs + '; // rename for clarity \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_AdvancedActivation(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    if layer_type(layer) == 'LeakyReLU':\n",
    "        s = 'k2c_LeakyReLU(' + inputs + '.array,' + inputs + '.numel,' + nm + '_alpha); \\n'\n",
    "    if layer_type(layer) == 'PReLU':\n",
    "        s = 'k2c_PReLU(' + inputs + '.array,' + inputs + '.numel,' + nm + '_alpha.array); \\n'\n",
    "    if layer_type(layer) == 'ELU':\n",
    "        s = 'k2c_ELU(' + inputs + '.array,' + inputs + '.numel,' + nm + '_alpha); \\n'\n",
    "    if layer_type(layer) == 'ThresholdedReLU':\n",
    "        s = 'k2c_ThresholdedReLU(' + inputs + '.array,' + inputs + '.numel,' + nm + '_theta); \\n'\n",
    "    if layer_type(layer) == 'ReLU':\n",
    "        s = 'k2c_ReLU(' + inputs + '.array,' + inputs + '.numel,' + nm + '_max_value, \\n\\t' + \\\n",
    "            nm + '_negative_slope,' + nm + '_threshold); \\n'\n",
    "    s += 'k2c_tensor *' + outputs + ' = ' + inputs + '; // rename for clarity \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_dummy_layer(layer,file,inputs,outputs,i):\n",
    "    s = 'k2c_tensor ' + outputs + ' = ' + inputs + \\\n",
    "        '; // layer only acts during training, during predict just rename for clarity \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_Reshape(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    s = 'k2c_reshape(' + inputs + ',' + nm + '_newshp,' + nm + '_newndim); \\n'\n",
    "    s += 'k2c_tensor *' + outputs + ' = ' + inputs + '; // rename for clarity \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_Flatten(layer,file,inputs,outputs,i):\n",
    "    s = 'k2c_flatten(' + inputs + '); \\n'\n",
    "    s += 'k2c_tensor *' + outputs + ' = ' + inputs + '; // rename for clarity \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_Permute(layer,file,inputs,outputs,i):\n",
    "    s = 'k2c_permute_dims(' + outputs + ',' + inputs + ',' + layer.name + '_permute); \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_RepeatVector(layer,file,inputs,outputs,i):\n",
    "    s = 'k2c_repeat_vector(' + outputs + ',' + inputs + ',' + layer.name + '_n); \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_Dot(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    s = 'k2c_dot(' + outputs + ',' + inputs[0] + ',' + inputs[1] + ',' + nm + '_axesA,' + nm + 'axesB,' + \\\n",
    "        nm + '_naxes,' + nm + '_normalize,' + nm + '_fwork); \\n'\n",
    "    file.write(s)\n",
    "      \n",
    "\n",
    "def layer2c(layer,file,inputs,outputs,i):\n",
    "\n",
    "    if layer_type(layer) == 'Dense':\n",
    "        write_layer_Dense(layer,file,inputs,outputs,i)\n",
    "\n",
    "    elif layer_type(layer) == 'LSTM':\n",
    "        write_layer_LSTM(layer,file,inputs,outputs,i)\n",
    "        \n",
    "    elif layer_type(layer) == 'GRU':\n",
    "        write_layer_GRU(layer,file,inputs,outputs,i)\n",
    "        \n",
    "    elif layer_type(layer) == 'SimpleRNN':\n",
    "        write_layer_SimpleRNN(layer,file,inputs,outputs,i)\n",
    "\n",
    "    elif layer_type(layer) == 'Conv1D':\n",
    "        write_layer_Conv1D(layer,file,inputs,outputs,i)\n",
    "    \n",
    "    elif layer_type(layer) in ['MaxPooling1D', 'AveragePooling1D']:\n",
    "        write_layer_Pooling1D(layer,file,inputs,outputs,i)\n",
    "    \n",
    "    elif layer_type(layer) in ['GlobalMaxPooling1D', 'GlobalAveragePooling1D']:\n",
    "        write_layer_GlobalPooling1D(layer,file,inputs,outputs,i)\n",
    "    \n",
    "    elif layer_type(layer) in ['Add','Subtract','Multiply','Average','Maximum','Minimum']:\n",
    "        write_layer_Merge(layer,file,inputs,outputs,i)\n",
    "    \n",
    "    elif layer_type(layer) == 'Activation':\n",
    "        write_layer_Activation(layer,file,inputs,outputs,i)\n",
    "    \n",
    "    elif layer_type(layer) in ['LeakyReLU','PReLU','ELU','ThresholdedReLU','ReLU']:\n",
    "        write_layer_AdvancedActivation(layer,file,inputs,outputs,i)\n",
    "    \n",
    "    elif layer_type(layer) == 'Reshape':\n",
    "        write_layer_Reshape(layer,file,inputs,outputs,i)\n",
    "        \n",
    "    elif layer_type(layer) == 'Flatten':\n",
    "        write_layer_Flatten(layer,file,inputs,outputs,i)\n",
    "        \n",
    "    elif layer_type(layer) in ['Dropout','SpatialDropout1D','SpatialDropout2D','SpatialDropout3D','ActivityRegularization',\\\n",
    "                               'GaussianNoise','GaussianDropout','AlphaDropout']:\n",
    "        write_dummy_layer(layer,file,inputs,outputs,i)\n",
    "    \n",
    "    elif layer_type(layer) == 'Permute':\n",
    "        write_layer_Permute(layer,file,inputs,outputs,i)\n",
    "    \n",
    "    elif layer_type(layer) == 'RepeatVector':\n",
    "        write_layer_RepeatVector(layer,file,inputs,outputs,i)\n",
    "        \n",
    "    elif layer_type(layer) == 'Dot':\n",
    "        write_layer_Dot(layer,file,inputs,outputs,i)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### types, names, io\n",
    "\n",
    "def layer_type(layer):\n",
    "    return str(layer.__class__).split('.')[-1][0:-2]\n",
    "\n",
    "def get_all_io_names(model):\n",
    "    a = [get_layer_io_names(layer) for layer in model.layers]\n",
    "    return list(set(flatten(a)))\n",
    "\n",
    "def get_layer_num_io(layer):\n",
    "    num_inputs = 0\n",
    "    error = False\n",
    "    while not error:\n",
    "        try:\n",
    "            layer.get_input_at(num_inputs)\n",
    "            num_inputs +=1\n",
    "        except ValueError:\n",
    "            error = True\n",
    "    \n",
    "    num_outputs = 0\n",
    "    error = False\n",
    "    while not error:\n",
    "        try:\n",
    "            layer.get_output_at(num_outputs)\n",
    "            num_outputs +=1\n",
    "        except ValueError:\n",
    "            error = True\n",
    "    return num_inputs, num_outputs\n",
    "\n",
    "def get_layer_io_names(layer):\n",
    "    num_inputs, num_outputs = get_layer_num_io(layer)\n",
    "    inputs = []\n",
    "    # num_inputs>1 -> shared layer\n",
    "    for i in range(num_inputs):\n",
    "        # is the input a list?\n",
    "        if isinstance(layer.get_input_at(i), list):\n",
    "            temp_list = []\n",
    "            list_length = len(layer.get_input_at(i))\n",
    "            for j in range(list_length):\n",
    "                name = str(layer.get_input_at(i)[j]).split()[0].split('\"')[1].split('/')[0].split(':')[0]\n",
    "                temp_list.append(name)\n",
    "            inputs.insert(i,temp_list)\n",
    "        else:\n",
    "            name = str(layer.get_input_at(i)).split()[0].split('\"')[1].split('/')[0].split(':')[0]\n",
    "            inputs.insert(i,name)\n",
    "    \n",
    "    outputs = []       \n",
    "    for i in range(num_outputs):\n",
    "        # is the output a list?\n",
    "        if isinstance(layer.get_output_at(i), list):\n",
    "            temp_list = []\n",
    "            list_length = len(layer.get_output_at(i))\n",
    "            for j in range(list_length):\n",
    "                name = str(layer.get_output_at(i)[j]).split()[0].split('\"')[1].split('/')[0].split(':')[0]\n",
    "                temp_list.append(name)\n",
    "            outputs.insert(i,temp_list)\n",
    "        else:\n",
    "            name = str(layer.get_output_at(i)).split()[0].split('\"')[1].split('/')[0].split(':')[0]\n",
    "            outputs.insert(i,name)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "def get_model_io_names(model):\n",
    "    num_inputs = len(model.inputs)\n",
    "    num_outputs = len(model.outputs)\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i in range(num_inputs):\n",
    "        nm = str(model.inputs[i]).split()[0].split('\"')[1].split('/')[0].split(':')[0]\n",
    "        inputs.append(nm)\n",
    "    for i in range(num_outputs):\n",
    "        nm = str(model.outputs[i]).split()[0].split('\"')[1].split('/')[0].split(':')[0]\n",
    "        outputs.append(nm)\n",
    "    return inputs, outputs\n",
    "\n",
    "def flatten(x):\n",
    "        if isinstance(x, list) or isinstance(x, tuple):\n",
    "            return [a for i in x for a in flatten(i)]\n",
    "        else:\n",
    "            return [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### model2c\n",
    "def model2c(model,file,function_name):\n",
    "    model_inputs, model_outputs = get_model_io_names(model)\n",
    "    \n",
    "    s = '#include <stdio.h> \\n#include <stddef.h> \\n#include <math.h> \\n#include <string.h> \\n'\n",
    "    s += '#include <stdarg.h> \\n#include \"k2c_include.h\" \\n'\n",
    "    s += '\\n \\n'\n",
    "    s += 'void ' + function_name + '('\n",
    "    s_in = ['k2c_tensor ' + in_nm + '_input' for in_nm in model_inputs]\n",
    "    s += ', '.join(s_in) + ', '\n",
    "    s_out = ['k2c_tensor ' + out_nm + '_output' for out_nm in model_outputs]\n",
    "    s += ', '.join(s_out) + ') { \\n \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "\n",
    "    print('Writing Weights')\n",
    "    for layer in model.layers:\n",
    "        weights2c(layer,file,[model_inputs,model_outputs])\n",
    "    written_io = set(model_inputs)\n",
    "    unwritten_io = set(get_all_io_names(model)) - written_io\n",
    "    \n",
    "    \n",
    "    while len(unwritten_io)>0:\n",
    "        for layer in model.layers:\n",
    "            layer_inputs, layer_outputs = get_layer_io_names(layer)\n",
    "            for i,(inp,outp) in enumerate(zip(layer_inputs,layer_outputs)):\n",
    "                if (set(flatten(inp)).issubset(written_io) and set(flatten(outp)).issubset(unwritten_io)) \\\n",
    "                    or layer_type(layer) == 'InputLayer':\n",
    "                    print('Writing layer ', outp)\n",
    "                    if set(flatten(inp)).issubset(set(model_inputs)):\n",
    "                        if isinstance(inp,list):\n",
    "                            inp_nm = [nm + '_input' for nm in inp]\n",
    "                        else:\n",
    "                            inp_nm = inp + '_input'\n",
    "                    else:\n",
    "                        if isinstance(inp,list):\n",
    "                            inp_nm = [nm + '_output' for nm in inp]\n",
    "                        else:\n",
    "                            inp_nm = inp + '_output'                    \n",
    "                    layer2c(layer,file,inp_nm,outp + '_output',i)\n",
    "                    written_io |= set(flatten(inp)) \n",
    "                    written_io |= set(flatten(outp))\n",
    "                    unwritten_io -= set(flatten(inp))\n",
    "                    unwritten_io -= set(flatten(outp))\n",
    "    file.write('\\n }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### keras2c\n",
    "def keras2c(model,function_name,num_tests=10):\n",
    "\n",
    "    function_name = str(function_name)\n",
    "    filename = function_name + '.h'\n",
    "    if isinstance(model,str):\n",
    "        model = keras.models.load_model(str(model_filepath))\n",
    "    elif not isinstance(model,keras.models.Model):\n",
    "        raise ValueError('Unknown model type. Model should either be an instance of keras.models.Model, or a filepath to a saved .h5 model')\n",
    "    \n",
    "    # check that the model can be converted\n",
    "    check_model(model, function_name)\n",
    "    print('All checks passed')\n",
    "    \n",
    "    file = open(filename,\"x+\")\n",
    "    model2c(model,file,function_name)\n",
    "    file.close()\n",
    "    make_test_suite(model,function_name,num_tests)\n",
    "    print(\"Done \\n C code is in '\" + function_name + \\\n",
    "          \".h' and tests are in '\" + function_name + \"_test_suite.c'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     11,
     42
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "### checks\n",
    "\n",
    "def is_valid_c_name(name):\n",
    "    allowed_chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_1234567890'\n",
    "    allowed_starting_chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_'\n",
    "    if not set(name).issubset(allowed_chars):\n",
    "        return False\n",
    "    if not set(name[0]).issubset(allowed_starting_chars):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def name_check(model):\n",
    "    valid = True\n",
    "    log = ''\n",
    "    for layer in model.layers:\n",
    "        if not is_valid_c_name(layer.name):\n",
    "            valid = False\n",
    "            log += \"layer name '\" + layer.name + \"' is not a valid C name \\n\"\n",
    "    return valid, log\n",
    "\n",
    "def layers_supported_check(model):\n",
    "    core_layers = ['Dense','Activation','InputLayer','Input','Dropout','SpatialDropout1D','SpatialDropout2D','SpatialDropout3D',\\\n",
    "                   'ActivityRegularization','Flatten','Reshape','Permute','RepeatVector']\n",
    "    conv_layers = ['Conv1D']\n",
    "    pool_layers = ['MaxPooling1D','AveragePooling1D','GlobalMaxPooling1D','GlobalAveragePooling1D']\n",
    "    local_layers = []\n",
    "    recur_layers = ['LSTM','GRU','SimpleRNN']\n",
    "    embed_layers = []\n",
    "    merge_layers = ['Add','Subtract','Multiply','Average','Maximum','Minimum']\n",
    "    activ_layers = ['LeakyReLU','PReLU','ELU','ThresholdedReLU','ReLU']\n",
    "    norm_layers = []\n",
    "    noise_layers = ['GaussianNoise','GaussianDropout','AlphaDropout']\n",
    "    \n",
    "    supported_layers = core_layers + conv_layers + pool_layers + local_layers + \\\n",
    "        recur_layers + embed_layers + merge_layers + activ_layers + norm_layers + noise_layers\n",
    "    valid = True\n",
    "    log = ''\n",
    "    for layer in model.layers:\n",
    "        if not (layer_type(layer) in supported_layers):\n",
    "            valid = False\n",
    "            log += \"layer type '\" + layer_type(layer) + \"' is not supported at this time \\n\"\n",
    "    return valid, log\n",
    "        \n",
    "def activation_supported_check(model):\n",
    "    supported_activations = ['linear', 'relu','softmax','softplus','softsign','relu','tanh',\\\n",
    "                             'sigmoid','hard_sigmoid','exponential' ]\n",
    "    valid = True\n",
    "    log = ''\n",
    "    for layer in model.layers:\n",
    "        if 'activation' in layer.get_config():\n",
    "            if not (layer.get_config()['activation'] in supported_activations):\n",
    "                valid = False\n",
    "                log += \"activation type '\" + layer.get_config()['activation'] + \\\n",
    "                    \"' for layer '\" + layer.name + \"' is not supported at this time \\n\"\n",
    "        if 'recurrent_activation' in layer.get_config():\n",
    "            if not (layer.get_config()['recurrent_activation'] in supported_activations):\n",
    "                valid = False\n",
    "                log += \"recurrent activation type '\" + layer.get_config()['recurrent_activation'] + \\\n",
    "                    \"' for layer '\" + layer.name + \"' is not supported at this time \\n\"\n",
    "    return valid, log\n",
    "\n",
    "# add check for masking\n",
    "def config_supported_check(model):\n",
    "    valid = True\n",
    "    log = ''\n",
    "    for layer in model.layers:\n",
    "        if 'data_format' in layer.get_config():\n",
    "            if layer.get_config()['data_format'] != 'channels_last':\n",
    "                valid = False\n",
    "                log += \"data format '\" + layer.get_config()['data_format'] + \"' for layer '\" + \\\n",
    "                    layer.name + \"' is not supported at this time \\n\"\n",
    "        if 'return_state' in layer.get_config():\n",
    "            if layer.get_config()['return_state']:\n",
    "                valid = False\n",
    "                log += \"'return_state' option for layer '\" + layer.name + \\\n",
    "                    \"' is not supported at this time \\n\"\n",
    "        if 'stateful' in layer.get_config():\n",
    "            if layer.get_config()['stateful']:\n",
    "                valid = False\n",
    "                log += \"'stateful' option for layer '\" + layer.name + \\\n",
    "                    \"' is not supported at this time \\n\"\n",
    "        if 'shared_axes' in layer.get_config():\n",
    "            if layer.get_config()['shared_axes'] is not None:\n",
    "                valid = False\n",
    "                log += \"shared axes option for layer '\" + layer.name + \\\n",
    "                    \"' is not supported at this time\"\n",
    "        if layer_type(layer) in ['Add','Subtract','Multiply','Average','Maximum','Minimum']:\n",
    "            inshps = layer.input_shape\n",
    "            insize = [np.prod(inp[1:]) for inp in inshps]\n",
    "            if len(set(insize)) >1:\n",
    "                valid=False\n",
    "                log += \"broadcasting merge functions between tensors of different shapes for layer '\" + layer.name + \"' is not currently supported\"\n",
    "    return valid, log\n",
    "\n",
    "def check_model(model, function_name):\n",
    "    valid_fname = True\n",
    "    log = 'The following errors were found: \\n'\n",
    "    if not is_valid_c_name(function_name):\n",
    "        valid_fname = False\n",
    "        log += \"function name '\" + function_name + \"' is not a valid C name \\n\"\n",
    "    valid_lname, name_log = name_check(model)\n",
    "    log += name_log\n",
    "    valid_layer, layer_log = layers_supported_check(model)\n",
    "    log += layer_log\n",
    "    valid_activation, activation_log = activation_supported_check(model)\n",
    "    log += activation_log\n",
    "    valid_config, config_log = config_supported_check(model)\n",
    "    log += config_log\n",
    "    if not (valid_fname and valid_lname and valid_layer and valid_activation and valid_config):\n",
    "        raise AssertionError(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### make test suite\n",
    "def make_test_suite(model,function_name,num_tests=10):\n",
    "    print('Writing tests')\n",
    "    input_shape = []\n",
    "    output_shape = []\n",
    "    model_inputs, model_outputs = get_model_io_names(model)\n",
    "    num_inputs = len(model_inputs)\n",
    "    num_outputs = len(model_outputs)\n",
    "    for i in range(num_inputs):\n",
    "        input_shape.insert(i,model.inputs[i].shape[1:])\n",
    "    for i in range(num_outputs):\n",
    "        output_shape.insert(i, model.outputs[i].shape[1:])\n",
    "    \n",
    "    \n",
    "    file = open(function_name + '_test_suite.c',\"x+\")\n",
    "    s = '#include <stdio.h> \\n#include <math.h> \\n#include <sys/time.h> \\n#include \"' + function_name + '.h\" \\n\\n'\n",
    "    s += 'float norm2(k2c_tensor *tensor1, k2c_tensor *tensor2);\\n'\n",
    "    s += 'struct timeval GetTimeStamp(); \\n \\n'\n",
    "    file.write(s)\n",
    "    s = 'int main(){\\n'\n",
    "    file.write(s)\n",
    "    for i in range(num_tests):\n",
    "        #generate random input and write to file\n",
    "        rand_inputs = []\n",
    "        for j,inpt in enumerate(model_inputs):\n",
    "            rand_input = np.random.random(input_shape[j])\n",
    "            file.write(array2c(rand_input,'test' + str(i+1) + '_' + model_inputs[j] + '_input'))\n",
    "            rand_input = rand_input[np.newaxis,...]\n",
    "            rand_inputs.insert(j,rand_input)\n",
    "        # make predictions\n",
    "        outputs = model.predict(rand_inputs)\n",
    "        # write predictions\n",
    "        if not isinstance(outputs,list):\n",
    "            outputs = [outputs]\n",
    "        for j,outpt in enumerate(model_outputs):\n",
    "            output = outputs[j][0,:]\n",
    "            file.write(array2c(output,'keras_' + model_outputs[j] + '_test' + str(i+1)))\n",
    "            file.write(array2c(np.zeros(output_shape[j]), 'c_' + model_outputs[j] + '_test' + str(i+1)))\n",
    "    s = ' float errors[' + str(num_tests*num_outputs) + '];\\n'\n",
    "    s += ' size_t num_tests = ' + str(num_tests) + '; \\n'\n",
    "    s += 'size_t num_outputs = ' + str(num_outputs) + '; \\n'\n",
    "    s += ' struct timeval t1 = GetTimeStamp(); \\n'\n",
    "    file.write(s)\n",
    "    for i in range(num_tests):\n",
    "        s = function_name + '('\n",
    "        for j, inpt in enumerate(model_inputs): \n",
    "            s +=  'test' + str(i+1) + '_' + model_inputs[j] + '_input,'\n",
    "        s += '\\n\\t'\n",
    "        for j, outpt in enumerate(model_outputs):\n",
    "            s += 'c_' + model_outputs[j] + '_test' + str(i+1) + ','\n",
    "        s = s[:-1] + '); \\n'\n",
    "        file.write(s)\n",
    "    file.write('\\n')\n",
    "    s =  'struct timeval t2 = GetTimeStamp(); \\n'\n",
    "    s += 'typedef unsigned long long u64; \\n'\n",
    "    s += 'u64 t1u = t1.tv_sec*1e6 + t1.tv_usec; \\n'\n",
    "    s += 'u64 t2u = t2.tv_sec*1e6 + t2.tv_usec; \\n'\n",
    "    s += 'printf(\"Average time over ' + str(num_tests) + ' tests: %llu us \\\\n\", (t2u-t1u)/' + str(num_tests) + '); \\n'\n",
    "    file.write(s)\n",
    "    for i in range(num_tests):\n",
    "        for j, outpt in enumerate(model_outputs):\n",
    "            s = 'errors[' + str(i*num_outputs+j) + '] = norm2(&keras_' + model_outputs[j] + '_test' + \\\n",
    "                str(i+1) + ',&c_' + model_outputs[j] + '_test' + str(i+1) + '); \\n'\n",
    "            file.write(s)\n",
    "    s = 'float maxerror = errors[0]; \\n'\n",
    "    s += 'for(size_t i=1; i< num_tests*num_outputs;i++){ \\n'\n",
    "    s += 'if (errors[i] > maxerror) { \\n'\n",
    "    s += 'maxerror = errors[i];}} \\n'\n",
    "    s += 'printf(\"Max L2 norm of output errors for ' + str(num_tests) + ' tests: %f \\\\n\", maxerror);\\n'\n",
    "    file.write(s)\n",
    "    s = 'if (maxerror > 1e-6) { \\n'\n",
    "    s += 'return 1;} \\n'\n",
    "    s += 'return 0;\\n} \\n\\n'\n",
    "    file.write(s)\n",
    "    s = \"\"\"float norm2(k2c_tensor *tensor1, k2c_tensor *tensor2){ \\n\n",
    "    float sum = 0; \\n\n",
    "    for(size_t i=0; i<tensor1->numel; i++){\\n\n",
    "    sum += (tensor1->array[i]-tensor2->array[i])*(tensor1->array[i]-tensor2->array[i]);}\\n\n",
    "    return sqrt(sum);}\\n\\n\"\"\"\n",
    "    file.write(s)\n",
    "    s = \"\"\"struct timeval GetTimeStamp() {\n",
    "    struct timeval tv;\n",
    "    gettimeofday(&tv,NULL);\n",
    "    return tv;}\"\"\"\n",
    "    file.write(s)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### test model\n",
    "inshape = (8,23,)\n",
    "inshape2 = (1,23)\n",
    "stride=1\n",
    "dilation=1\n",
    "filter_height=3\n",
    "num_filters=5\n",
    "pad = 'valid'\n",
    "a = keras.layers.Input(inshape)\n",
    "b = keras.layers.Dense(48)(a)\n",
    "m = keras.layers.Dropout(.4)(b)\n",
    "c = keras.layers.Dense(30)(m)\n",
    "d = keras.layers.Conv1D(filters=num_filters, kernel_size=filter_height, strides=stride, padding=pad, \\\n",
    "                        dilation_rate=dilation)(c)\n",
    "e = keras.layers.Dense(20)(d)\n",
    "f = keras.layers.GRU(20, activation='relu')(e)\n",
    "g = keras.layers.Dense(20)(f)\n",
    "h = keras.layers.Input(inshape2)\n",
    "i = keras.layers.Dense(20)(h)\n",
    "j = keras.layers.MaxPooling1D(pool_size=2,padding='same', strides=1)(i)\n",
    "k = keras.layers.Add()([j,g])\n",
    "l = keras.layers.Dense(30)(k)\n",
    "                       \n",
    "model = keras.models.Model(inputs=[h], outputs=[j])\n",
    "# model.save('test1.h5')\n",
    "# keras2c('test1.h5','test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inshp = (12, 46)\n",
    "units = 17\n",
    "a = keras.layers.Input(inshp)\n",
    "b = keras.layers.GRU(units, activation='softmax',\n",
    "                     recurrent_activation='softplus',\n",
    "                     go_backwards=True,\n",
    "                     return_sequences=True,\n",
    "                     reset_after=True)(a)\n",
    "model = keras.models.Model(inputs=a, outputs=b)\n",
    "\n",
    "\n",
    "input_shape = []\n",
    "output_shape = []\n",
    "model_inputs, model_outputs = get_model_io_names(model)\n",
    "num_inputs = len(model_inputs)\n",
    "num_outputs = len(model_outputs)\n",
    "for i in range(num_inputs):\n",
    "    input_shape.insert(i,model.inputs[i].shape[0:])\n",
    "for i in range(num_outputs):\n",
    "    output_shape.insert(i, model.outputs[i].shape[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.layers[1]\n",
    "weights = layer.get_weights()\n",
    "if layer.get_config()['use_bias']:\n",
    "    bias = weights[2]\n",
    "    if layer.get_config()['reset_after']:\n",
    "        rbias = bias[1]\n",
    "        bias = bias[0]\n",
    "    else:\n",
    "        bias = bias\n",
    "        rbias = np.zeros(3*units)\n",
    "else:\n",
    "    bias = np.zeros(3*units)\n",
    "    rbias = np.zeros(3*units)\n",
    "bias = np.concatenate([bias, rbias], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777.6207999999999\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "inp1 = np.random.random(inshape)\n",
    "inp2 = np.random.random(inshape2)\n",
    "inp = [inp1[np.newaxis,...],inp2[np.newaxis,...]]\n",
    "num_tests = 10\n",
    "t1 = time.perf_counter_ns()\n",
    "for i in range(num_tests):\n",
    "    outp = model.predict(inp)\n",
    "t2 = time.perf_counter_ns()\n",
    "print((t2-t1)/1000/num_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['gcc', '-o', 'foo', 'model1_test_suite.c', '-lm'], returncode=0, stdout=b'', stderr=b'')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run(['gcc','-o','foo','model1_test_suite.c','-lm'],capture_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'gcc: error: model1_test_suite.c -o model1_test -lm: No such file or directory\\ngcc: fatal error: no input files\\ncompilation terminated.\\n'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "layers:  embedding\n",
    "         batch normalization\n",
    "         concatentate\n",
    "         bidirectional rnn\n",
    "         merge with different sizes\n",
    "         conv2d\n",
    "         pool2d \n",
    "         conv3d\n",
    "         pool3d\n",
    "         global pool 2d\n",
    "         global pool 3d\n",
    "         seperable conv\n",
    "         conv transpose\n",
    "         depthwise conv\n",
    "         crop1d\n",
    "         crop2d\n",
    "         crop3d\n",
    "         pad2d\n",
    "         pad3d\n",
    "         upsampling1d\n",
    "         upsampling2d\n",
    "         upsampling3d\n",
    "         locally connected 1d\n",
    "         locally connected 2d\n",
    "         time distributed\n",
    " \n",
    " test all the layers\n",
    " static or const?\n",
    " broadcasting sizes for merge layers\n",
    " pass in array of pointers to arrays and array of sizes?\n",
    " find the common dimension and add them that way?\n",
    " tests return 1 if norm > 1e-6\n",
    " set up travis/coverage/pytest/gcov\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
